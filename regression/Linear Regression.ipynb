{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Stochastic Linear Regression\n",
    "\n",
    "\n",
    "### Execução\n",
    "O programa abaixo quando executado fará a regressão linear do dataset bike_sharing.csv e retornará a média dos erros quadráticos da regressão e a respectiva matriz de pesos. Para efeito de comparação, será retornado também a média dos erros quadráticos da implementaçao do SKLEARN.\n",
    "\n",
    "Por padrão o algoritmo é o estocástico e a normalização não será feita (na discussão abaixo tem um detalhamento). \n",
    "O conjunto de treino tem 70% dos registros e o outros 30% foram utilizados para teste (predição). \n",
    "O dataset é randomizado a cada execução do algoritmo. \n",
    "\n",
    "Se necessário, na função ```main``` existem dois parâmetros ```s_alpha``` e ```s_epochs``` que são respectivamente a taxa de aprendizado alfa e o número de iterações (épocas), podendo ser ajustadas a escolha do usuário. \n",
    "\n",
    "Na inicilização do objeto da classe LR (classe que foi implementada que realiza a regressão), pode-se especificar dois parâmetros ```algorithm``` e ```normalize```. Os valores padrão são ```algorithm ='stochastic'``` e ```normalize = False```, sendo possível a mudança para ```algorithm ='batch'``` e ```normalize = True``` se o usuário desejar mudar o algoritmo para batch ou para normalizar os dados. Convém lembrar que ao mudar o algoritmo ou a normalização, é necessário ajustar os parâmetros ```s_alpha``` e ```s_epochs```. O gráfico que foi gerado após a regressão pode ajudar a ajustar esses parâmetros como discutido abaixo.\n",
    "\n",
    "### Discussão\n",
    "Os parâmetros alfa e número de iterações foram encontrados a partir da execução do programa para vários valores dos mesmos e analisando o gráfico do custo total pelo número de iterações. O algoritmo também foi comparado com a implementação do SKLEARN. Os resultados foram os seguintes:\n",
    "\n",
    "    Para os dados não-normalizados, com um alfa = 0.000001, a regressão linear converge bem em apenas uma iteração. Para chegar a essa conclusão, foi analisado o gráfico mencionado acima com até 50 iterações para verificar em qual iteração a regressão tendia a convergência. Se o valor de alfa aumentar, a regressão não vai convergir. O valor do score é bem próximo da implementação original do SKLEARN.\n",
    "\n",
    "    Para os dados normalizados, a situação foi diferente. Com um alfa = 0.0001, a regressão converge bem em pelo menos 10 iterações, porém o score é geralmente um pouco maior que a implementação do SKLEARN. Se o alfa aumentar, a regressão não converge. Se diminuir, são necessárias mais iterações, por exemplo, com um alfa = 0.00001 são necessárias pelo menos 50 iterações para se perceber a convergência.\n",
    "\n",
    "\n",
    "Os valores dos pesos da regressão mostram a importância de cada atributo na predição. Nesse caso os valores dos pesos tendem a serem baixos e parecidos, da ordem de $10^{-2}$, então podemos concluir que os atributos tendem a ter uma importância quase igual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error of SKLEARN regression: 64.640289\n",
      "\n",
      "\n",
      "\n",
      "Mean squared error of Leo's stochastic regression (alpha=0.000001, epochs=1): 64.868324\n",
      "\n",
      " Weights matrix\n",
      "[  4.08905624e-03   1.15412429e-02   3.86366194e-03   2.36214067e-02\n",
      "   6.31769394e-02   4.11751919e-06   1.07896817e-02   9.79378392e-04\n",
      "   2.84474765e-03   4.43785775e-03  -4.45028766e-04   1.20792060e-03\n",
      "  -1.15809876e-03   1.00194864e+00   9.94599931e-01]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEPCAYAAACp/QjLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFvlJREFUeJzt3XuQZGV5x/Hvsy7iGhcW5OJls44k0VwMGZGoCVVh45VL\nXGIZI6YIjjFlTGJCAnLRUCEkBpVcwGBuFOpgKCNqGcqkLAJehlyUKLIsigaIAQNRkURQBDQsPvmj\nz+y22z2zPdPn9OnzzvdT1UWfS/d5f/TsPHPep/t0ZCaSJPVb1/YAJEnTx+IgSRpgcZAkDbA4SJIG\nWBwkSQMsDpKkARMpDhGxLiKuj4gP7rH+ooi4bxJjkCSNblJnDqcAn+tfERHPAPYH/KCFJE2ZxotD\nRGwGjgMu6Vu3Dvgj4PSmjy9JWrlJnDlcQK8I9J8hvBa4IjPvAmICY5AkrUCjxSEijgfuyswbqIpA\nRDweeCnwtiaPLUlavWjy2koRcR5wErAT2ABsBL5d3b5Fr2BsAb6QmU8Z8nj7EZK0Cpk51qxMo2cO\nmfmGzNySmYcBJwIfzczHZuYTMvOwzHwy8MCwwtD3HMXezjnnnNbHYDbzma+8Wx2m4XMOa/bs4Pbb\nb297CI0pORuYr+tKz1eH9ZM6UGZeA1wzZP1+kxqDJGk003DmsGbNzc21PYTGlJwNzNd1peerQ6MN\n6XFFRE7z+CRpGkUEOc0NaS1vYWGh7SE0puRsYL6uKz1fHSwOkqQBTitJUmGcVpIkNcLi0KKS5z1L\nzgbm67rS89XB4iBJGmDPQZIKY89BktQIi0OLSp73LDkbmK/rSs9XB4uDJGmAPQdJKow9B0lSIywO\nLSp53rPkbGC+ris9Xx0sDpKkAfYcJKkw9hwkSY2wOLSo5HnPkrOB+bqu9Hx1sDhIkgbYc5Ckwthz\nkCQ1wuLQopLnPUvOBubrutLz1cHiIEkaYM9Bkgpjz0GS1AiLQ4tKnvcsORuYr+tKz1cHi4MkaYA9\nB0kqjD0HSVIjLA4tKnnes+RsYL6uKz1fHSwOkqQB9hwkqTD2HCRJjbA4tKjkec+Ss4H5uq70fHWw\nOEiSBkyk5xAR64BPA3dk5raIuAQ4stp8CzCXmQ8MeZw9B0laoS71HE4Bbupb/q3MnM3MWeAO4LUT\nGockaQSNF4eI2AwcB1yyuC4zv1ltC2ADsCZPD0qe9yw5G5iv60rPV4dJnDlcAJzOHgUgIt4BfBl4\nKnDRBMYhSRpRoz2HiDgeODYzXxsRW4HTMvNFfduDXmG4LjPnhzw+X/GKVzAzMwPApk2bmJ2dZevW\nrcDu6u+yyy67vJaXFxYWmJ+fB2BmZoZzzz137J5D08XhPOAkYCe96aONwAcy8+S+fX4KeF1mbhvy\neBvSkrRCU9+Qzsw3ZOaWzDwMOBH4aGaeHBHfB7vOHF4E/HuT45hWi5W/RCVnA/N1Xen56rB+0ges\nCsKlEbERCGAH8KuTHockaWleW0mSCjP100qSpG6yOLSo5HnPkrOB+bqu9Hx1sDhIkgbYc5Ckwthz\nkCQ1wuLQopLnPUvOBubrutLz1cHiIEkaYM9Bkgpjz0GS1AiLQ4tKnvcsORuYr+tKz1cHi4MkaYA9\nB0kqjD0HSVIjLA4tKnnes+RsYL6uKz1fHSwOkqQB9hwkqTD2HCRJjbA4tKjkec+Ss4H5uq70fHWw\nOEiSBthzkKTC2HOQJDXC4tCikuc9S84G5uu60vPVweIgSRpgz0GSCmPPQZLUCItDi0qe9yw5G5iv\n60rPVweLgyRpgD0HSSqMPQdJUiMsDi0qed6z5Gxgvq4rPV8dLA6SpAH2HCSpMPYcJEmNsDi0qOR5\nz5Kzgfm6rvR8dbA4SJIGTKTnEBHrgE8Dd2Tmtoi4DDgS+D/gk8CvZObDQx5nz0GSVqhLPYdTgJv6\nli/LzB/MzMOBRwO/PKFxSJJG0HhxiIjNwHHAJYvrMvPKvl0+CWxuehzTqOR5z5Kzgfm6rvR8dZjE\nmcMFwOnAwPxQRKwHfhG4cs9tkqT27LXnEBHzmTm3t3VLPPZ44NjMfG1EbAVOy8wX9W2/GPhmZp66\nxOPtOUjSCtXRc1g/wj6H73HQdcCPj/j8RwHbIuI4YAOwMSLelZknR8Q5wEGZ+erlnmBubo6ZmRkA\nNm3axOzsLFu3bgV2nxq67LLLLq/l5YWFBebn5wF2/b4c15JnDhFxJnAWsBH4xuJqetNDb8/M01d0\noIij6Z05bIuIXwZeCTwnM7+9zGOKPnNYWFjY9UKXpuRsYL6uKz1f0+9WOh84mF7P4ODqdlBmHrjS\nwjDEXwKHANdGxPURcfaYzydJqtEoPYdnAzdm5gMR8XLg6cBFmXlH44Mr/MxBkpowqc85XAw8GBGH\nA2cC/w1cNs5BJUnTbZTisLP68/0E4G2Z+VZ6fQiNabGhVKKSs4H5uq70fHUY5d1K90fE6fQ+j3B0\n9W6lfZodliSpTaP0HJ4AnAR8KjM/FhFbgOdm5jsbH5w9B0lasTp6DiNdeC8iDqJ3oTyA6zLzf8Y5\n6KgsDpK0chNpSEfES4Dr6U0rnQxcFxEvHueg6il53rPkbGC+ris9Xx1G6Tn8LvDjmXkXQEQcClwF\n/F2TA5MktWeUnsNnMvNH+5bXATv61zU2OKeVJGnFJnVtpasi4kPAu6vlE+mdOUiSCjXK5xxeB8wD\nzwSeBVxardOYSp73LDkbmK/rSs9XhyXPHCLiMODQzPwE8N7qRkT8JDAD3DaJAUqSJm+5q7L+PXB2\nZu7YY/3hwB9k5gmND86egyStWNNvZX3cnoUBIDNvBJ48zkElSdNtueKw/zLbHl33QNaikuc9S84G\n5uu60vPVYbnisD0iXrnnyoiYA7Y3NiJJUuuW6zk8HrgCuA/4dLX6SHpXZD0hM7/c+ODsOUjSik3k\n2koR8XzgadXiTZk5sc84WBwkaeUmcm2lzLw6My+obn74rUYlz3uWnA3M13Wl56vDKB+CkyStMSNd\nsrstTitJ0spN6jukJUlrzJLFISLuiYivDbndExFfm+QgS1XyvGfJ2cB8XVd6vjosd1XWgyY2CknS\nVBm55xARBwKPWlzOzC81Nai+Y9pzkKQVmtTXhB4fEbcAdwL/Vv33o+McVJI03UZpSP8hcBRwc2Z+\nL/BC4J8bHdUaUfK8Z8nZwHxdV3q+OoxSHHZm5t3AuujN81xN74t/JEmFGuXyGR8BtgHnA/sBXwWO\nysxnNz44ew6StGKTurbSRuABemcZJ9O7lPe7MvN/xjnwSIOzOEjSik3qQ3Cvz8yHM/OhzHx7Zv4p\ncOo4B1VPyfOeJWcD83Vd6fnqMEpxOGbIuuPrHogkaXos930OvwK8BngKcHPfpo3AdZn58sYH57SS\nJK1Yoz2HiDgAeCzwJuCsvk33ZeZXxznoqCwOkrRyjfYcMvOezPyPzHwpsAF4fnU7eJwDareS5z1L\nzgbm67rS89VhlE9I/zrwXmBLdXtvRPxa0wOTJLVnlLey3gj8ZGZ+s1p+DPDxzDy88cE5rSRJKzap\nt7IG8FDf8kPVupFFxLqI2B4RH6yWfz0ibo2Ih6sL+kmSpshy3+eweDnvvwGujYizI+Js4OPApSs8\nzinATX3L/wI8F/jiCp+nKCXPe5acDczXdaXnq8NyZw6fBMjM84FX0/uU9IPAazLzj0c9QERsBo4D\nLllcl5k7MvO/WOEZiCRpMpZ7K+v2zHz62AeIeB+9K7vuD5yWmdv6tt0GPCMzh36znD0HSVq5OnoO\ny30T3MERseRlMqrLaCwrIo4H7srMGyJiK54pSFInLFccHgE8hvF+oR8FbIuI4+h9VmJjRLwrM0+u\ntu/1tGBubo6ZmRkANm3axOzsLFu3bgV2zxt2dfnCCy8sKk//cv+c7jSMx3zmKznfwsIC8/PzALt+\nX45ruWml6zPziFqO0nu+oxk+rXRkZv7vEo8pelppYWFh1wtdmpKzgfm6rvR8TV8+o5aeQ9/z7SoO\nEfEbwBnAofS+H+JDmfnqIY8pujhIUhOaLg4HLtUonhSLgyStXNPXVmq1MKwF/fOepSk5G5iv60rP\nV4dRPiEtSVpj9nptpTY5rSRJKzepaytJktYYi0OLSp73LDkbmK/rSs9XB4uDJGmAPQdJKow9B0lS\nIywOLSp53rPkbGC+ris9Xx0sDpKkAfYcJKkw9hwkSY2wOLSo5HnPkrOB+bqu9Hx1sDhIkgbYc5Ck\nwthzkCQ1wuLQopLnPUvOBubrutLz1cHiIEkaYM9Bkgpjz0GS1AiLQ4tKnvcsORuYr+tKz1cHi4Mk\naYA9B0kqjD0HSVIjLA4tKnnes+RsYL6uKz1fHSwOkqQB9hwkqTD2HCRJjbA4tKjkec+Ss4H5uq70\nfHWwOEiSBthzkKTC2HOQJDXC4tCikuc9S84G5uu60vPVweIgSRpgz0GSCmPPQZLUiIkUh4hYFxHX\nR8QHq+WZiLg2Im6OiL+NiPWTGMe0KXnes+RsYL6uKz1fHSZ15nAK8Lm+5bcAf5KZTwXuBV41oXFI\nkkbQeM8hIjYD7wT+EDg1M7dFxN3AoZn5nYh4NvB7mXnMkMfac5CkFepKz+EC4HQgASLiscA9mfmd\navudwBMmMA5J0oganeuPiOOBuzLzhojYuri6uvVb8vRgbm6OmZkZADZt2sTs7Cxbt/aeanHesKvL\nF154YVF5+pf753SnYTzmM1/J+RYWFpifnwfY9ftyXI1OK0XEecBJwE5gA7ARuAJ4AfC4vmmlczLz\n2CGPL3paaWFhYdcLXZqSs4H5uq70fHVMK03scw4RcTRwWtVzuBz4QGZeHhF/CezIzL8a8piii4Mk\nNaErPYdhzgJOjYhbgAOBt7c0DknSEBMrDpl5TWZuq+7flpnPysynZObLMvOhSY1jmvTPe5am5Gxg\nvq4rPV8d/IS0JGmA11aSpMJ0uecgSZpiFocWlTzvWXI2MF/XlZ6vDhYHSdIAew6SVBh7DpKkRlgc\nWlTyvGfJ2cB8XVd6vjpYHCRJA+w5SFJh7DlIkhphcWhRyfOeJWcD83Vd6fnqYHGQJA2w5yBJhbHn\nIElqhMWhRSXPe5acDczXdaXnq4PFQZI0wJ6DJBXGnoMkqREWhxaVPO9ZcjYwX9eVnq8OFgdJ0gB7\nDpJUGHsOkqRGWBxaVPK8Z8nZwHxdV3q+OlgcJEkD7DlIUmHsOUiSGmFxaFHJ854lZwPzdV3p+epg\ncZAkDbDnIEmFsecgSWqExaFFJc97lpwNzNd1peerg8VBkjTAnoMkFcaegySpEY0Wh4jYNyL+LSK2\nR8RnIuKcav1zIuLTEXFjRLwzItZkkSp53rPkbGC+ris9Xx0a/aWcmd8Gfjoznw7MAsdGxE8A88DP\nZ+bhwBeBuSbHMa1uuOGGtofQmJKzgfm6rvR8dWj8L/bMfKC6uy+wHtgJfCszv1Ct/zDwkqbHMY3u\nvffetofQmJKzgfm6rvR8dWi8OETEuojYDnwFuDozPwXsExFHVLv8HLC56XFIkkY3iTOH71TTSpuB\nZ0bEDwMnAhdGxLXAN+idTaw5t99+e9tDaEzJ2cB8XVd6vjpM9K2sEfG7wDcz80/71j0feFVmnjhk\nf9/HKkmrMO5bWdfXNZBhIuIg4KHM/HpEbACeB7w5Ig7OzLsjYl/gTOCNwx4/bjhJ0uo0WhyAxwOX\nVm9VXQdcnpkfiojzI+JngAD+IjMXGh6HJGkFpvoT0pKkdrT+4bOIOCAiroqImyPiHyNi/yX2e0VE\n3FLtd3Lf+n0i4q+r9Z+LiBdPbvR7N26+vu0fjIgbmx/x6MbJFhEbIuIfIuLz1Qckz5vs6JcWEcdE\nxL9XYz5zyPZHRsR7IuLWiPhERGzp2/b6av3nI+IFkx35aFabLyKeFxHXRcSOiPhURPz05Ee/vHFe\nu2r7loi4LyJOndyoRzfmz+bhEfHxiPhs9Ro+ctmDZWarN+AtwBnV/TOBNw/Z5wDgC8D+wKbF+9W2\n3wN+v2/fA9vOVGe+avuLgcuAG9vOU1c2YANwdLXPeuCfgBdOQaZ1wH8ATwL2AW4AfnCPfX6V3nQo\nwMuA91T3fxjYXuWZqZ4n2s5UY74fAx5X3f8R4M6289SVrW/7+4HLgVPbzlPza/cIYAfwtGr5gL39\nbLZ+5gCcAFxa3b8U+Nkh+7wQuCozv56Z9wJXAcdU234JeNPijpn5tQbHuhpj5YuI7wF+myWa9i1b\ndbbMfDAzrwHIzJ3A9UzH512eCdyamV/MzIeA99DL2a8/9/uB51T3t9H7x7gzM28Hbq2eb5qsJt9z\nATJzR2Z+pbp/E7BvROwzmWGPZNXZACLiBHp/vNw0gbGuxjg/my8AdmTmZwEy856sqsRSpqE4HJKZ\ndwFUP3gHD9nnicAdfcv/DTyxbxrjjdW1mi6PiGGPb9Oq81X3/wD4Y+DBJge5SuNmAyAiNgEvAj7S\n0DhXYs/x3ske4+3fJzMfBr4eEQcOeexA1imwmnz3Vvl2iYifA7ZXv6SmxaqzRcSjgTOAc+m9UWYa\njfOz+RSAiLiymho8fW8Ha/rdSlQDuho4tH8VkMDZoz7FkHVJb/ybgX/OzNMi4reBPwEG5uyb1FS+\niPgx4Psz89SImFliv0Y1+NotPv8jgHcDF1Z/bbdt2fHuZZ9RHtu21eQLvvs1+xF6Z+vPr3doYxsn\n27nABZn5QEQs9VxtGyffeuAo4EjgW8BHIuK6zPzYUgebSHHIzCV/iCLirog4NDPviojHAV8dstud\nwNa+5c3AxzLzfyPi/sy8olr/PnrTTBPVVD7gJ4AjIuI/6c0xHhIRH83M5ww+RTMazLboYuDmzLyo\njvHW4E6gv0m5GfjSHvvcAXwv8KWquO2fmfdExJ3V+uUe27bV5NsvM+8BiIjNwAeAX5ySYt5v1dki\n4lnASyLifHrz8Q9HxIOZ+ReTGPiIxsl3J3BN3+v4IeAIvvvf4nebgibLW4Azq/ujNDUX72+qtr2b\n3pVfoXd118vbzlRnvr59nsR0NqTHee3eCLyv7Rx7jPcR7G76PZJe0++H9tjn19jd9DuRwYb0I4En\nM50N6XHybar2f3HbOerOtsc+5zCdDelxX7vrgEfROym4Gjh22eNNQeAD6V2Z9eZqwIu/OJ4BXNy3\n3xy9Bt8twMl967cA11T/o64GNredqc58fdunsTisOhu9udHv0Gv+bafXkP6ltjNVYzumynQrcFa1\n7lzgZ6r7+wLvrbZfC8z0Pfb11T/gzwMvaDtLnfmA3wHuq16rxdfsoLbz1PXa9T3HVBaHGn42fwH4\nLHAj8Ka9HcsPwUmSBkzDu5UkSVPG4iBJGmBxkCQNsDhIkgZYHCRJAywOkqQBFgetORHxcERcHxHb\nq/+eUeNzPykiPlPX80ltmcjlM6Qpc39mHtHg8/vhIXWeZw5ai4ZeVC0ibouIt0TEjRFxbUQcVq3f\nEhEfjogbIuLq6vpCRMQhEfGBav32iHh29VTrI+Li6ktVrozed6UTEb8ZETdV+797IkmlVbI4aC3a\nsMe00kv7tt2TmYcDfw68tVr3NmA+M2fpXctr8SKBfwYsVOuPYPf3APwAcFFmPg34OvCSav2ZwGy1\n/2uaCifVwctnaM2JiG9k5n5D1t9G7yKOt0fEeuDLmXlwRNxN7xvQHq7WfykzD4mIrwJPzL7vNIiI\nJ9H7cqOnVstnAOsz87zqSpj3A1cAV2Tm/c2nlVbHMwfpu+US95faZ5hv991/mN29vePpnYUcAXwq\nIvz3p6nlD6fWouW+yOVl1X9PBD5R3f9X4OXV/ZOAf6nuf5jeJZKJiHURsXEvz78le1+NehawH/CY\nlQ9dmgzfraS16FERcT27vyXrysx8Q7XtgIjYQe/bshYLwinAOyLidcDdwCur9b8FXBwRrwJ20vty\n968w5Myimo66LCL2q4771sz8RiPppBrYc5AqVc/hGZn5tbbHIrXNaSVpN/9SkiqeOUiSBnjmIEka\nYHGQJA2wOEiSBlgcJEkDLA6SpAEWB0nSgP8HzRCjYNYWwfkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa931a132d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats as ss\n",
    "# from bokeh.plotting import figure, output_file, show, save\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
    "from pylab import *\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "\n",
    "\n",
    "class LR:\n",
    "\n",
    "    predictions = []\n",
    "    weights = []\n",
    "    cost = []\n",
    "\n",
    "    def __init__(self, alpha, epochs, algorithm='stochastic', normalize=True):\n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "        self.algorithm = algorithm\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def add_column_of_ones(self, X):\n",
    "        rows = len(X)\n",
    "        arrayOfOnes = np.ones(rows)\n",
    "        X = np.c_[arrayOfOnes, X]  # add column of ones\n",
    "        return X\n",
    "\n",
    "    def normalization(self, X):\n",
    "        # scaler = MinMaxScaler()\n",
    "        # scaler.fit(X)\n",
    "        # X = scaler.transform(X)\n",
    "        X = ss.zscore(X)\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.normalize:\n",
    "            X = self.normalization(X)\n",
    "        X = self.add_column_of_ones(X)\n",
    "        self.predictions = X.dot(self.weights)\n",
    "        return self.predictions\n",
    "\n",
    "    def r_squared(self, X, y, weights):\n",
    "        return np.sum(np.power((X.dot(weights) - y), 2)) / (2 * len(X))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.algorithm == 'stochastic':\n",
    "            self.stochastic(X,y)\n",
    "        elif self.algorithm == 'batch':\n",
    "            self.batch(X, y)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def stochastic(self, X, y):\n",
    "        if self.normalize:\n",
    "            X = self.normalization(X)\n",
    "        X = self.add_column_of_ones(X)\n",
    "        size, features = X.shape\n",
    "        weights = np.zeros(features)\n",
    "        cost = np.zeros(self.epochs)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            for index, row in enumerate(X):\n",
    "                error = row.dot(weights) - y[index]\n",
    "                weights = weights - self.alpha * error * row\n",
    "            cost[i] = self.r_squared(X, y, weights)\n",
    "\n",
    "        self.weights = weights[:]\n",
    "        self.cost = cost[:]\n",
    "\n",
    "    def batch(self, X, y):\n",
    "        if self.normalize:\n",
    "            X = self.normalization(X)\n",
    "        X = self.add_column_of_ones(X)\n",
    "        size, features = X.shape\n",
    "        weights = np.zeros(features)\n",
    "        cost = np.zeros(self.epochs)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            error = X.dot(weights) - y\n",
    "            weights = weights - (self.alpha/size)*error.dot(X)\n",
    "            cost[i] = self.r_squared(X, y, weights)\n",
    "\n",
    "        self.weights = weights[:]\n",
    "        self.cost = cost[:]\n",
    "\n",
    "\n",
    "def get_data():\n",
    "\n",
    "    # load data\n",
    "    data = np.loadtxt(\"bike_sharing.csv\", delimiter=\",\", skiprows=1)\n",
    "\n",
    "    # randomize rows and split features from labels\n",
    "    ndata = np.random.permutation(data)\n",
    "    columns = ndata.shape[1]\n",
    "    nt = int(math.floor(len(ndata) * 0.7))\n",
    "    trainingFeatures = ndata[0:nt, 0:columns - 1]\n",
    "    trainingLabels = ndata[0:nt, -1]\n",
    "    testFeatures = ndata[nt:, 0:columns - 1]\n",
    "    testLabels = ndata[nt:, -1]\n",
    "\n",
    "    return trainingFeatures, trainingLabels, testFeatures, testLabels\n",
    "\n",
    "\n",
    "def main():\n",
    "    trainingFeatures, trainingLabels, testFeatures, testLabels = get_data()\n",
    "\n",
    "    # SKLEARN LINEAR REGRESSION\n",
    "    regr = linear_model.LinearRegression(normalize=False)\n",
    "    regr.fit(trainingFeatures, trainingLabels)\n",
    "#     print regr.intercept_\n",
    "#     print regr.coef_\n",
    "    print(\"Mean squared error of SKLEARN regression: %f\"\n",
    "          % np.mean((regr.predict(testFeatures) - testLabels) ** 2))\n",
    "    print '\\n\\n'\n",
    "\n",
    "    # LEO's LINEAR REGRESSION\n",
    "    s_alpha = 0.000001\n",
    "    s_epochs = 1\n",
    "    stochastic_regression = LR(s_alpha, s_epochs, algorithm='stochastic', normalize=False)\n",
    "    stochastic_regression.fit(trainingFeatures, trainingLabels)\n",
    "    print(\"Mean squared error of Leo's stochastic regression (alpha=%f, epochs=%d): %f\"\n",
    "              % (s_alpha,  s_epochs, np.mean((stochastic_regression.predict(testFeatures) - testLabels) ** 2)))\n",
    "    print '\\n Weights matrix'\n",
    "    print stochastic_regression.weights\n",
    "\n",
    "\n",
    "    # Graph cost vs epochs\n",
    "#     it = np.arange(s_epochs)\n",
    "#     p = figure(x_axis_label='Epochs', y_axis_label='Cost')\n",
    "#     p.line(it, stochastic_regression.cost, line_width=2)\n",
    "#     show(p)\n",
    "\n",
    "    it = np.arange(s_epochs)\n",
    "    plot(it, stochastic_regression.cost)\n",
    "    xlabel('Epochs')\n",
    "    ylabel('Total Cost')\n",
    "    grid(True)\n",
    "    show()\n",
    "\n",
    "    # b_alpha = 0.01\n",
    "    # b_epochs = 500\n",
    "    # batch_regression = LR(b_alpha, b_epochs)\n",
    "    # batch_regression.batch(trainingFeatures, trainingLabels)\n",
    "    # print batch_regression.weights\n",
    "    # print(\"Mean squared error batch regression (alpha=%f, epochs=%d): %f\"\n",
    "    #       % (b_alpha, b_epochs, np.mean((batch_regression.predict(testFeatures) - testLabels) ** 2)))\n",
    "    #\n",
    "    # it = np.arange(b_epochs)\n",
    "    # a = figure(x_axis_label='Iterations', y_axis_label='Cost')\n",
    "    # a.line(it, batch_regression.cost, line_width=2)\n",
    "    # show(a)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
